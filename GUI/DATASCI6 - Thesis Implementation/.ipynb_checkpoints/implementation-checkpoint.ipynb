{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebb7b3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load models and necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be5de6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Kirby\n",
      "[nltk_data]     Wenceslao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Imports for the models\n",
    "#import necessary libraries\n",
    "\n",
    "#preprocessing\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import validators\n",
    "import emoji\n",
    "import unidecode\n",
    "import nltk\n",
    "import pickle\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "\n",
    "#model loading\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from itertools import chain\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "from keras import Input\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "import torch\n",
    "from keras import regularizers\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import make_blobs\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
    "from keras.utils import np_utils\n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from itertools import chain\n",
    "\n",
    "from tqdm import tqdm\n",
    "from gensim.models import fasttext\n",
    "from gensim.test.utils import datapath\n",
    "import os, re, csv, math, codecs, pickle, nltk\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e680562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing necessary variables,lists,objects, and functions\n",
    "#Define necessary functions\n",
    "\n",
    "\n",
    "#Functions for the gui\n",
    "\n",
    "def classify():\n",
    "    start_time = time.time()\n",
    "    user_input = inputtxt.get('1.0',END)\n",
    "    print(user_input)\n",
    "    tokenized_tweet = pre_process_tweet(user_input)\n",
    "    \n",
    "    #classification\n",
    "    \n",
    "    #binary classifications\n",
    "    bf_7030.set(fasttextcnn_predict(tokenized_tweet, binary_7030_tokenizer, max_sentence_len_binary_7030, \n",
    "                                    binary_7030_fasttext_model, 0))\n",
    "    bt_7030.set(tfidfffnn_predict(tokenized_tweet, binary_7030_tfidf, binary_7030_selector, binary_7030_tfidf_model, 0))\n",
    "    \n",
    "    bf_8020.set(fasttextcnn_predict(tokenized_tweet, binary_8020_tokenizer, max_sentence_len_binary_8020, \n",
    "                                    binary_8020_fasttext_model, 0))\n",
    "    bt_8020.set(tfidfffnn_predict(tokenized_tweet, binary_8020_tfidf, binary_8020_selector, binary_8020_tfidf_model, 0))\n",
    "    \n",
    "    bf_9010.set(fasttextcnn_predict(tokenized_tweet, binary_9010_tokenizer, max_sentence_len_binary_9010, \n",
    "                                    binary_9010_fasttext_model, 0))\n",
    "    bt_9010.set(tfidfffnn_predict(tokenized_tweet, binary_9010_tfidf, binary_9010_selector, binary_9010_tfidf_model, 0))\n",
    "    \n",
    "    #multilabel classifications\n",
    "    \n",
    "    mf_7030m.set(fasttextcnn_predict(tokenized_tweet, multilabel_7030_tokenizer, max_sentence_len_multilabel_7030, \n",
    "                                    multilabel_7030_fasttext_model, 1))\n",
    "    mt_7030m.set(tfidfffnn_predict(tokenized_tweet, multilabel_7030_tfidf, multilabel_7030_selector, \n",
    "                                   multilabel_7030_tfidf_model, 1))\n",
    "    \n",
    "    mf_8020m.set(fasttextcnn_predict(tokenized_tweet, multilabel_8020_tokenizer, max_sentence_len_multilabel_8020, \n",
    "                                    multilabel_8020_fasttext_model, 1))\n",
    "    mt_8020m.set(tfidfffnn_predict(tokenized_tweet, multilabel_8020_tfidf, multilabel_8020_selector, \n",
    "                                   multilabel_8020_tfidf_model, 1))\n",
    "    \n",
    "    mf_9010m.set(fasttextcnn_predict(tokenized_tweet, multilabel_9010_tokenizer, max_sentence_len_multilabel_9010, \n",
    "                                    multilabel_9010_fasttext_model, 1))\n",
    "    mt_9010m.set(tfidfffnn_predict(tokenized_tweet, multilabel_9010_tfidf, multilabel_9010_selector, \n",
    "                                   multilabel_9010_tfidf_model, 1))\n",
    "    \n",
    "    run_time =  time.time() - start_time\n",
    "    print(\"Runtime: \",run_time)\n",
    "    \n",
    "def clear():\n",
    "    inputtxt.delete('1.0', END)\n",
    "    step_1.set('')\n",
    "    step_2.set('')\n",
    "    step_3.set('')\n",
    "    step_4.set('')\n",
    "    step_5.set('')\n",
    "    step_6.set('')\n",
    "    step_7.set('')\n",
    "    \n",
    "    bf_7030.set('')\n",
    "    bt_7030.set('')\n",
    "    bf_8020.set('')\n",
    "    bt_8020.set('')\n",
    "    bf_9010.set('')\n",
    "    bt_9010.set('')\n",
    "    \n",
    "    mf_7030m.set('')\n",
    "    mt_7030m.set('')\n",
    "    mf_8020m.set('')\n",
    "    mt_8020m.set('')\n",
    "    mf_9010m.set('')\n",
    "    mt_9010m.set('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20f47189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre Processing\n",
    "\n",
    "#Declare Stop Words\n",
    "\n",
    "filipino_stopwords = set(\n",
    "    \"\"\"\n",
    "akin\n",
    "aking\n",
    "ako\n",
    "alin\n",
    "am\n",
    "amin\n",
    "aming\n",
    "ang\n",
    "ano\n",
    "anumang\n",
    "apat\n",
    "at\n",
    "atin\n",
    "ating\n",
    "ay\n",
    "bababa\n",
    "bago\n",
    "bakit\n",
    "bawat\n",
    "bilang\n",
    "dahil\n",
    "dalawa\n",
    "dapat\n",
    "din\n",
    "dito\n",
    "doon\n",
    "gagawin\n",
    "gayunman\n",
    "ginagawa\n",
    "ginawa\n",
    "ginawang\n",
    "gumawa\n",
    "gusto\n",
    "habang\n",
    "hanggang\n",
    "hindi\n",
    "huwag\n",
    "iba\n",
    "ibaba\n",
    "ibabaw\n",
    "ibig\n",
    "ikaw\n",
    "ilagay\n",
    "ilalim\n",
    "ilan\n",
    "inyong\n",
    "isa\n",
    "isang\n",
    "itaas\n",
    "ito\n",
    "iyo\n",
    "iyon\n",
    "iyong\n",
    "ka\n",
    "kahit\n",
    "kailangan\n",
    "kailanman\n",
    "kami\n",
    "kanila\n",
    "kanilang\n",
    "kanino\n",
    "kanya\n",
    "kanyang\n",
    "kapag\n",
    "kapwa\n",
    "karamihan\n",
    "katiyakan\n",
    "katulad\n",
    "kaya\n",
    "kaysa\n",
    "ko\n",
    "kong\n",
    "kulang\n",
    "kumuha\n",
    "kung\n",
    "laban\n",
    "lahat\n",
    "lamang\n",
    "likod\n",
    "lima\n",
    "maaari\n",
    "maaaring\n",
    "maging\n",
    "mahusay\n",
    "makita\n",
    "marami\n",
    "marapat\n",
    "masyado\n",
    "may\n",
    "mayroon\n",
    "mga\n",
    "minsan\n",
    "mismo\n",
    "mula\n",
    "muli\n",
    "na\n",
    "nabanggit\n",
    "naging\n",
    "nagkaroon\n",
    "nais\n",
    "nakita\n",
    "namin\n",
    "napaka\n",
    "narito\n",
    "nasaan\n",
    "ng\n",
    "ngayon\n",
    "ni\n",
    "nila\n",
    "nilang\n",
    "nito\n",
    "niya\n",
    "niyang\n",
    "noon\n",
    "o\n",
    "pa\n",
    "paano\n",
    "pababa\n",
    "paggawa\n",
    "pagitan\n",
    "pagkakaroon\n",
    "pagkatapos\n",
    "palabas\n",
    "pamamagitan\n",
    "panahon\n",
    "pangalawa\n",
    "para\n",
    "paraan\n",
    "pareho\n",
    "pataas\n",
    "pero\n",
    "pumunta\n",
    "pumupunta\n",
    "sa\n",
    "saan\n",
    "sabi\n",
    "sabihin\n",
    "sarili\n",
    "sila\n",
    "sino\n",
    "siya\n",
    "tatlo\n",
    "tayo\n",
    "tulad\n",
    "tungkol\n",
    "una\n",
    "walang\n",
    "\"\"\".split()\n",
    ")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "search = \"leni robredo bongbong marcos isko moreno domagoso manny pacman pacquiao ping lacson ernie abella leody de guzman norberto gonzales jose montemayor jr faisal mangondato\"\n",
    "candidatelist = search.split(\" \")\n",
    "\n",
    "#pre-process tweet input\n",
    "def pre_process_tweet(tweet_input):\n",
    "    \n",
    "    #Step 1 - Extract Tweet from input\n",
    "    #Tweet = tweet_input\n",
    "    tweet = tweet_input.strip().replace(\"\\n\",\" \")\n",
    "    step_1.set(tweet)\n",
    "    \n",
    "    #Step 2 - Data Deidentification\n",
    "    output = \"\"\n",
    "    sentence = tweet.split(\" \")\n",
    "    for part in sentence:\n",
    "        if not re.match(r\"(^|[^@\\w])@(\\w{1,15})\\b\", part):\n",
    "            if len(output) == 0:\n",
    "                output = f\"{part}\"\n",
    "            else:\n",
    "                output = f\"{output} {part}\"\n",
    "\n",
    "    tweets_de_identified = output\n",
    "    step_2.set(tweets_de_identified)\n",
    "    \n",
    "    #Step 3 - URL Removal\n",
    "    output = \"\"\n",
    "    sentence = tweets_de_identified.split(\" \")\n",
    "    for part in sentence:\n",
    "        valid = validators.url(part)\n",
    "\n",
    "        if (not valid == True):\n",
    "            if len(output) == 0:\n",
    "                output = f\"{part}\"\n",
    "            else:\n",
    "                output = f\"{output} {part}\"\n",
    "                \n",
    "    tweets_url_removed = output\n",
    "    step_3.set(tweets_url_removed)\n",
    "    \n",
    "    #Step 4 - Special Character Processing\n",
    "    \n",
    "    emoji_removed = emoji.replace_emoji(tweets_url_removed, replace=\"[emoji]\")\n",
    "    output = \"\"\n",
    "    sentence = emoji_removed.split(\" \")\n",
    "    \n",
    "    for part in sentence:\n",
    "        if not (re.match(r\"^[_\\W]+$\", part) or \"[emoji]\" in part):\n",
    "            if len(output) == 0:\n",
    "                output = f\"{part}\"\n",
    "            else:\n",
    "                output = f\"{output} {part}\"\n",
    "    \n",
    "    tweets_specialcharacters_removed = output\n",
    "    step_4.set(tweets_specialcharacters_removed)\n",
    "    \n",
    "    #Step 5 - Normalization, lowercase>removediacritics>remove numerics and symbols>stopwords\n",
    "    \n",
    "    #lowercase the text\n",
    "    lowercased_input = tweets_specialcharacters_removed.lower()\n",
    "\n",
    "    #remove diacritics\n",
    "    diacritics_removed = unidecode.unidecode(lowercased_input)\n",
    "\n",
    "    output = \"\"\n",
    "    sentence = diacritics_removed.split(\" \")\n",
    "\n",
    "    for part in sentence:\n",
    "        part = re.sub(\"[^A-Za-z ]+$\", \"\", part)\n",
    "        part = re.sub(\"^[^A-Za-z #]+\", \"\", part)\n",
    "        if not (len(part) <= 1 or re.match(r\"[^#a-zA-Z]\", part) or part in english_stopwords or \n",
    "                part in filipino_stopwords or any(part in x for x in candidatelist)):     \n",
    "            if len(output) == 0:\n",
    "                output = f\"{part}\"\n",
    "            else:\n",
    "                output = f\"{output} {part}\"  \n",
    "                \n",
    "    tweets_normalized = output\n",
    "    step_5.set(tweets_normalized)\n",
    "    \n",
    "    #Step 6 - Hashtag Processing, removing the hashtags from the tweet\n",
    "    output = \"\"\n",
    "    sentence = tweets_normalized.split(\" \")\n",
    "\n",
    "    for part in sentence:\n",
    "        if not re.match(r\"#(\\w+)\", part):\n",
    "            if len(output) == 0:\n",
    "                output = f\"{part}\"\n",
    "            else:\n",
    "                output = f\"{output} {part}\"\n",
    "                \n",
    "    tweets_hashtags_removed = output  \n",
    "    step_6.set(tweets_hashtags_removed)\n",
    "    #Step 7 - Tokenization\n",
    "    tokenizer = WhitespaceTokenizer()\n",
    "    \n",
    "    output = tokenizer.tokenize(tweets_hashtags_removed)\n",
    "    \n",
    "    tweets_tokenized = output\n",
    "    tokens = ','.join(str(s) for s in tweets_tokenized)\n",
    "    \n",
    "    step_7.set(tokens)\n",
    "    \n",
    "    return tweets_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f013ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load necessary files for models\n",
    "\n",
    "#Load Models and files\n",
    "\n",
    "#x\n",
    "with open('binary/7030/x_train.pkl', 'rb') as file:\n",
    "    binary_7030_x_train = pickle.load(file)\n",
    "    \n",
    "    \n",
    "with open('binary/8020/x_train.pkl', 'rb') as file:\n",
    "    binary_8020_x_train = pickle.load(file)\n",
    "\n",
    "    \n",
    "with open('binary/9010/x_train.pkl', 'rb') as file:\n",
    "    binary_9010_x_train = pickle.load(file)\n",
    "    \n",
    "    \n",
    "with open('multilabel/7030/x_train.pkl', 'rb') as file:\n",
    "    multilabel_7030_x_train = pickle.load(file)\n",
    "    \n",
    "    \n",
    "with open('multilabel/8020/x_train.pkl', 'rb') as file:\n",
    "    multilabel_8020_x_train = pickle.load(file)\n",
    "    \n",
    "    \n",
    "with open('multilabel/9010/x_train.pkl', 'rb') as file:\n",
    "    multilabel_9010_x_train = pickle.load(file)\n",
    "    \n",
    "#y\n",
    "with open('binary/7030/y_train.pkl', 'rb') as file:\n",
    "    binary_7030_y_train = pickle.load(file)\n",
    "    \n",
    "#Load Models and files\n",
    "    \n",
    "with open('binary/8020/y_train.pkl', 'rb') as file:\n",
    "    binary_8020_y_train = pickle.load(file)\n",
    "\n",
    "#Load Models and files\n",
    "    \n",
    "with open('binary/9010/y_train.pkl', 'rb') as file:\n",
    "    binary_9010_y_train = pickle.load(file)\n",
    "    \n",
    "#Load Models and files\n",
    "    \n",
    "with open('multilabel/7030/y_train.pkl', 'rb') as file:\n",
    "    multilabel_7030_y_train = pickle.load(file)\n",
    "    \n",
    "#Load Models and files\n",
    "    \n",
    "with open('multilabel/8020/y_train.pkl', 'rb') as file:\n",
    "    multilabel_8020_y_train = pickle.load(file)\n",
    "    \n",
    "#Load Models and files\n",
    "    \n",
    "with open('multilabel/9010/y_train.pkl', 'rb') as file:\n",
    "    multilabel_9010_y_train = pickle.load(file)\n",
    "    \n",
    "\n",
    "    \n",
    "#for fasttext cnn\n",
    "binary_7030_fit = list(chain.from_iterable(binary_7030_x_train))\n",
    "binary_8020_fit = list(chain.from_iterable(binary_8020_x_train))\n",
    "binary_9010_fit = list(chain.from_iterable(binary_9010_x_train))\n",
    "\n",
    "multilabel_7030_fit = list(chain.from_iterable(multilabel_7030_x_train))\n",
    "multilabel_8020_fit = list(chain.from_iterable(multilabel_8020_x_train))\n",
    "multilabel_9010_fit = list(chain.from_iterable(multilabel_9010_x_train))\n",
    "\n",
    "\n",
    "list_len = [len(i) for i in binary_7030_x_train]\n",
    "index_of_max = np.argmax(np.array(list_len))\n",
    "max_sentence_len_binary_7030 = list_len[index_of_max]\n",
    "\n",
    "list_len = [len(i) for i in binary_8020_x_train]\n",
    "index_of_max = np.argmax(np.array(list_len))\n",
    "max_sentence_len_binary_8020 = list_len[index_of_max]\n",
    "\n",
    "list_len = [len(i) for i in binary_9010_x_train]\n",
    "index_of_max = np.argmax(np.array(list_len))\n",
    "max_sentence_len_binary_9010 = list_len[index_of_max]\n",
    "\n",
    "list_len = [len(i) for i in multilabel_7030_x_train]\n",
    "index_of_max = np.argmax(np.array(list_len))\n",
    "max_sentence_len_multilabel_7030 = list_len[index_of_max]\n",
    "\n",
    "list_len = [len(i) for i in multilabel_8020_x_train]\n",
    "index_of_max = np.argmax(np.array(list_len))\n",
    "max_sentence_len_multilabel_8020 = list_len[index_of_max]\n",
    "\n",
    "list_len = [len(i) for i in multilabel_9010_x_train]\n",
    "index_of_max = np.argmax(np.array(list_len))\n",
    "max_sentence_len_multilabel_9010 = list_len[index_of_max]\n",
    "\n",
    "#feed train set on vectorizer \n",
    "binary_7030_tokenizer = Tokenizer(num_words=100000, char_level=False)\n",
    "binary_7030_tokenizer.fit_on_texts(binary_7030_fit)\n",
    "\n",
    "binary_8020_tokenizer = Tokenizer(num_words=100000, char_level=False)\n",
    "binary_8020_tokenizer.fit_on_texts(binary_8020_fit)\n",
    "\n",
    "binary_9010_tokenizer = Tokenizer(num_words=100000, char_level=False)\n",
    "binary_9010_tokenizer.fit_on_texts(binary_9010_fit)\n",
    "\n",
    "multilabel_7030_tokenizer = Tokenizer(num_words=100000, char_level=False)\n",
    "multilabel_7030_tokenizer.fit_on_texts(multilabel_7030_fit)\n",
    "\n",
    "multilabel_8020_tokenizer = Tokenizer(num_words=100000, char_level=False)\n",
    "multilabel_8020_tokenizer.fit_on_texts(multilabel_8020_fit)\n",
    "\n",
    "multilabel_9010_tokenizer = Tokenizer(num_words=100000, char_level=False)\n",
    "multilabel_9010_tokenizer.fit_on_texts(multilabel_9010_fit)\n",
    "\n",
    "\n",
    "#vectorizer and tfidf for tfidf models\n",
    "\n",
    "#Convert Labels from Strings to categorical Integers {Non-Hate = 1, Hate = 0}\n",
    "mapping_binary = {'Non-hate': 0, 'Hate': 1}\n",
    "mapping_multilabel = {'Positive': 0, 'Negative': 1, 'Neutral':2}\n",
    "\n",
    "binary_7030_df_ytrain = pd.DataFrame(binary_7030_y_train, columns = ['Label'])\n",
    "binary_8020_df_ytrain = pd.DataFrame(binary_8020_y_train, columns = ['Label'])\n",
    "binary_9010_df_ytrain = pd.DataFrame(binary_9010_y_train, columns = ['Label'])\n",
    "\n",
    "multilabel_7030_df_ytrain = pd.DataFrame(multilabel_7030_y_train, columns = ['Label'])\n",
    "multilabel_8020_df_ytrain = pd.DataFrame(multilabel_8020_y_train, columns = ['Label'])\n",
    "multilabel_9010_df_ytrain = pd.DataFrame(multilabel_9010_y_train, columns = ['Label'])\n",
    "\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "binary_7030_df_ytrain = pd.DataFrame(binary_7030_y_train, columns = ['Label'])\n",
    "binary_7030_df_ytrain = binary_7030_df_ytrain.replace({'Label': mapping_binary})\n",
    "binary_7030_train_y = binary_7030_df_ytrain['Label'].tolist()\n",
    "\n",
    "binary_7030_tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None)  \n",
    "\n",
    "#classifier building/ fitting of training dataset to tfidf\n",
    "binary_7030_fitted_training_x = binary_7030_tfidf.fit_transform(binary_7030_x_train)\n",
    "\n",
    "#transform based on top 40 percent features\n",
    "binary_7030_selector = SelectPercentile(f_classif, percentile = 40)\n",
    "binary_7030_selector.fit(binary_7030_fitted_training_x, binary_7030_train_y)\n",
    "\n",
    "binary_8020_df_ytrain = pd.DataFrame(binary_8020_y_train, columns = ['Label'])\n",
    "binary_8020_df_ytrain = binary_8020_df_ytrain.replace({'Label': mapping_binary})\n",
    "binary_8020_train_y = binary_8020_df_ytrain['Label'].tolist()\n",
    "\n",
    "binary_8020_tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None)  \n",
    "\n",
    "#classifier building/ fitting of training dataset to tfidf\n",
    "binary_8020_fitted_training_x = binary_8020_tfidf.fit_transform(binary_8020_x_train)\n",
    "\n",
    "#transform based on top 40 percent features\n",
    "binary_8020_selector = SelectPercentile(f_classif, percentile = 40)\n",
    "binary_8020_selector.fit(binary_8020_fitted_training_x, binary_8020_train_y)\n",
    "\n",
    "binary_9010_df_ytrain = pd.DataFrame(binary_9010_y_train, columns = ['Label'])\n",
    "binary_9010_df_ytrain = binary_9010_df_ytrain.replace({'Label': mapping_binary})\n",
    "binary_9010_train_y = binary_9010_df_ytrain['Label'].tolist()\n",
    "\n",
    "binary_9010_tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None)  \n",
    "\n",
    "#classifier building/ fitting of training dataset to tfidf\n",
    "binary_9010_fitted_training_x = binary_9010_tfidf.fit_transform(binary_9010_x_train)\n",
    "\n",
    "#transform based on top 40 percent features\n",
    "binary_9010_selector = SelectPercentile(f_classif, percentile = 40)\n",
    "binary_9010_selector.fit(binary_9010_fitted_training_x, binary_9010_train_y)\n",
    "\n",
    "multilabel_7030_df_ytrain = pd.DataFrame(multilabel_7030_y_train, columns = ['Label'])\n",
    "multilabel_7030_df_ytrain = multilabel_7030_df_ytrain.replace({'Label': mapping_multilabel})\n",
    "multilabel_7030_train_y = multilabel_7030_df_ytrain['Label'].tolist()\n",
    "\n",
    "multilabel_7030_tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None)  \n",
    "\n",
    "#classifier building/ fitting of training dataset to tfidf\n",
    "multilabel_7030_fitted_training_x = multilabel_7030_tfidf.fit_transform(multilabel_7030_x_train)\n",
    "\n",
    "#transform based on top 40 percent features\n",
    "multilabel_7030_selector = SelectPercentile(f_classif, percentile = 40)\n",
    "multilabel_7030_selector.fit(multilabel_7030_fitted_training_x, multilabel_7030_train_y)\n",
    "\n",
    "multilabel_8020_df_ytrain = pd.DataFrame(multilabel_8020_y_train, columns = ['Label'])\n",
    "multilabel_8020_df_ytrain = multilabel_8020_df_ytrain.replace({'Label': mapping_multilabel})\n",
    "multilabel_8020_train_y = multilabel_8020_df_ytrain['Label'].tolist()\n",
    "\n",
    "multilabel_8020_tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None)  \n",
    "\n",
    "#classifier building/ fitting of training dataset to tfidf\n",
    "multilabel_8020_fitted_training_x = multilabel_8020_tfidf.fit_transform(multilabel_8020_x_train)\n",
    "\n",
    "#transform based on top 40 percent features\n",
    "multilabel_8020_selector = SelectPercentile(f_classif, percentile = 40)\n",
    "multilabel_8020_selector.fit(multilabel_8020_fitted_training_x, multilabel_8020_train_y)\n",
    "\n",
    "multilabel_9010_df_ytrain = pd.DataFrame(multilabel_9010_y_train, columns = ['Label'])\n",
    "multilabel_9010_df_ytrain = multilabel_9010_df_ytrain.replace({'Label': mapping_multilabel})\n",
    "multilabel_9010_train_y = multilabel_9010_df_ytrain['Label'].tolist()\n",
    "\n",
    "multilabel_9010_tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None)  \n",
    "\n",
    "#classifier building/ fitting of training dataset to tfidf\n",
    "multilabel_9010_fitted_training_x = multilabel_9010_tfidf.fit_transform(multilabel_9010_x_train)\n",
    "\n",
    "#transform based on top 40 percent features\n",
    "multilabel_9010_selector = SelectPercentile(f_classif, percentile = 40)\n",
    "multilabel_9010_selector.fit(multilabel_9010_fitted_training_x, multilabel_9010_train_y)\n",
    "\n",
    "#load fasttext cnn models\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "binary_7030_fasttext_model = load_model(\"binary/7030/fasttextcnn.h5\")\n",
    "binary_8020_fasttext_model = load_model(\"binary/8020/fasttextcnn.h5\")\n",
    "binary_9010_fasttext_model = load_model(\"binary/9010/fasttextcnn.h5\")\n",
    "multilabel_7030_fasttext_model = load_model(\"multilabel/7030/fasttextcnn.h5\")\n",
    "multilabel_8020_fasttext_model = load_model(\"multilabel/8020/fasttextcnn.h5\")\n",
    "multilabel_9010_fasttext_model = load_model(\"multilabel/9010/fasttextcnn.h5\")\n",
    "\n",
    "#load tfidf ffnn models\n",
    "binary_7030_tfidf_model = load_model(\"binary/7030/cabasag_model.h5\")\n",
    "binary_8020_tfidf_model = load_model(\"binary/8020/cabasag_model.h5\")\n",
    "binary_9010_tfidf_model = load_model(\"binary/9010/cabasag_model.h5\")\n",
    "multilabel_7030_tfidf_model = load_model(\"multilabel/7030/cabasag_model.h5\")\n",
    "multilabel_8020_tfidf_model = load_model(\"multilabel/8020/cabasag_model.h5\")\n",
    "multilabel_9010_tfidf_model = load_model(\"multilabel/9010/cabasag_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "267fd4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction functions\n",
    "\n",
    "def fasttextcnn_predict(preprocessed_tweet, tokenizer, max_len, model, classification_type):\n",
    "    output = ''\n",
    "    tweet = \" \".join(preprocessed_tweet)\n",
    "    transformed = tokenizer.texts_to_sequences([tweet])\n",
    "    padded = keras.preprocessing.sequence.pad_sequences(transformed, maxlen=max_len)\n",
    "\n",
    "    if(classification_type == 0): \n",
    "        prediction = model.predict(padded)\n",
    "        array = prediction.ravel()\n",
    "        \n",
    "        '\\033[1m' + 'Python' + '\\033[0m'\n",
    "        if(array[0] > 0.5):\n",
    "            output =\"[{classification}: {value}]\".format(classification = 'Hate', value = '{:.4f}'.format(array[0]))\n",
    "        else:\n",
    "            output = \"[{classification}: {value}]\".format(classification = 'Non-hate', value = '{:.4f}'.format(array[0]))\n",
    "    else:\n",
    "        prediction = model.predict(padded)\n",
    "        array = prediction.ravel()\n",
    "        highest = np.argmax(array)\n",
    "        \n",
    "        if highest == 0:\n",
    "            output = \"[{classification}: {value}], {classification2}: {value2}, {classification3}: {value3}\".format(classification = 'Positive', value = '{:.4f}'.format(array[0]),\n",
    "                                                                                                         classification2 = 'Negative', value2 ='{:.4f}'.format(array[1]),\n",
    "                                                                                                         classification3 = 'Neutral', value3 = '{:.4f}'.format(array[2]))\n",
    "        elif highest == 1:\n",
    "            output = \"{classification}: {value}, [{classification2}: {value2}], {classification3}: {value3}\".format(classification = 'Positive', value = '{:.4f}'.format(array[0]),\n",
    "                                                                                                         classification2 = 'Negative', value2 ='{:.4f}'.format(array[1]),\n",
    "                                                                                                         classification3 = 'Neutral', value3 = '{:.4f}'.format(array[2]))\n",
    "        elif highest == 2:\n",
    "            output = \"{classification}: {value}, {classification2}: {value2}, [{classification3}: {value3}]\".format(classification = 'Positive', value = '{:.4f}'.format(array[0]),\n",
    "                                                                                                         classification2 = 'Negative', value2 ='{:.4f}'.format(array[1]),\n",
    "                                                                                                         classification3 = 'Neutral', value3 = '{:.4f}'.format(array[2]))\n",
    "                \n",
    "    return output\n",
    "\n",
    "def tfidfffnn_predict(preprocessed_tweet, tfidf, selector, model, classification_type):\n",
    "    output = ''\n",
    "    \n",
    "    transformed = tfidf.transform([preprocessed_tweet])\n",
    "    vectorized_tweet = selector.transform(transformed).toarray()\n",
    "    \n",
    "    if(classification_type == 0): \n",
    "        prediction = model.predict(vectorized_tweet)\n",
    "        array = prediction.ravel()\n",
    "        highest = np.argmax(array)\n",
    "        \n",
    "        if highest == 0:\n",
    "            output = \"[{classification}: {value}], {classification2}: {value2}\".format(classification = 'Non-hate', value = '{:.4f}'.format(array[0]), \n",
    "                                                                             classification2 = 'Hate', value2 = '{:.4f}'.format(array[1]))\n",
    "        elif highest == 1:\n",
    "            output = \"{classification}: {value}, [{classification2}: {value2}]\".format(classification = 'Non-hate', value = '{:.4f}'.format(array[0]), \n",
    "                                                                         classification2 = 'Hate', value2 = '{:.4f}'.format(array[1]))         \n",
    "    else:\n",
    "        prediction = model.predict(vectorized_tweet)\n",
    "        array = prediction.ravel()\n",
    "        highest = np.argmax(array)\n",
    "        \n",
    "        if highest == 0:\n",
    "            output = \"[{classification}: {value}], {classification2}: {value2}, {classification3}: {value3}\".format(classification = 'Positive', value = '{:.4f}'.format(array[0]),\n",
    "                                                                                                         classification2 = 'Negative', value2 ='{:.4f}'.format(array[1]),\n",
    "                                                                                                         classification3 = 'Neutral', value3 = '{:.4f}'.format(array[2]))\n",
    "        elif highest == 1:\n",
    "            output = \"{classification}: {value}, [{classification2}: {value2}], {classification3}: {value3}\".format(classification = 'Positive', value = '{:.4f}'.format(array[0]),\n",
    "                                                                                                         classification2 = 'Negative', value2 ='{:.4f}'.format(array[1]),\n",
    "                                                                                                         classification3 = 'Neutral', value3 = '{:.4f}'.format(array[2]))\n",
    "        elif highest == 2:\n",
    "            output = \"{classification}: {value}, {classification2}: {value2}, [{classification3}: {value3}]\".format(classification = 'Positive', value = '{:.4f}'.format(array[0]),\n",
    "                                                                                                         classification2 = 'Negative', value2 ='{:.4f}'.format(array[1]),\n",
    "                                                                                                         classification3 = 'Neutral', value3 = '{:.4f}'.format(array[2]))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e75720b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cafde38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@edupunay @PhilippineStar Hahaha ang nenega talaga ng kakampinks! Hindi nalang tanggapin na panalo na si Bongbong Marcos number 7 sa balota!! Hindi kami nag papabrainwash kay Leni Lugaw. 😛 Solid BBM kami! Solid UNITEAM ❤️💚 #BBMIsMyPresident2022 #BBMSARA2022 https://t.co/CaYxkEeKCH\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kirby Wenceslao\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime:  4.086027383804321\n",
      "@edupunay @PhilippineStar Hahaha ang nenega talaga ng kakampinks! Hindi nalang tanggapin na panalo na si Bongbong Marcos number 7 sa balota!! Hindi kami nag papabrainwash kay Leni Lugaw. 😛 Solid BBM kami! Solid UNITEAM ❤️💚 #BBMIsMyPresident2022 #BBMSARA2022 https://t.co/CaYxkEeKCH\n",
      "\n",
      "Runtime:  0.32999181747436523\n",
      "🔮 president leni robredo on may 9 cutie 🔮\n",
      "\n",
      "Runtime:  0.31794238090515137\n"
     ]
    }
   ],
   "source": [
    "from tkinter import*\n",
    "from tkinter import ttk  # Normal Tkinter.* widgets are not themed!\n",
    "from ttkthemes import ThemedTk\n",
    "    \n",
    "thesis_gui = ThemedTk(theme=\"adapta\")\n",
    "\n",
    "thesis_gui.geometry(\"1200x900\")\n",
    "thesis_gui.configure(background = \"GhostWhite\")\n",
    "thesis_gui.title(\"Philippine Election Related Tweets Sentiment Analysis\")\n",
    "thesis_gui.resizable(False, True)\n",
    "\n",
    "style = ttk.Style()\n",
    "style.configure(\"Bold.TLabel\", font=(\"Helvetica\", 10, \"bold\"))\n",
    "style.configure(\"Bold2.TLabel\", font=(\"Helvetica\", 9, \"bold\"))\n",
    "style.configure(\"Bold3.TLabel\", font=(\"Helvetica\", 8, \"bold\"))\n",
    "style.configure(\"Bold.TButton\", font=(\"Helvetica\", 10, \"bold\"))\n",
    "\n",
    "title = Label(thesis_gui, font = ('Helvetica 14 bold'), \n",
    "              bg = \"GhostWhite\",\n",
    "              text = \"Hate Speech in Filipino Election-Related Tweets: A Sentiment Analysis Using Convolutional Neural Networks\")\n",
    "title.pack(pady = 7)\n",
    "\n",
    "names = Label(thesis_gui, font = ('Helvetica 12 italic'), \n",
    "              bg = \"GhostWhite\",\n",
    "              text = \"Argañosa, Marasigan, Villanueva, & Wenceslao. 2022\")\n",
    "names.pack()\n",
    "#Input Label Frame\n",
    "frame_label = ttk.Label(text=\"INPUT TWEET HERE\", style=\"Bold.TLabel\")\n",
    "input_frame = ttk.LabelFrame(thesis_gui, \n",
    "                         labelwidget = frame_label,\n",
    "                         relief='ridge')\n",
    "input_frame.pack(fill=\"both\", padx = 5, pady= 5)\n",
    "\n",
    "inputtxt = Text(input_frame, height = 4,\n",
    "                width = 200,\n",
    "                font = (\"Helvetica 11\"),\n",
    "                bg = \"white\")\n",
    "inputtxt.pack( padx = 6, pady= 7)\n",
    "\n",
    "#Preprocessing Label Frame\n",
    "frame_label = ttk.Label(text=\"PREPROCESSING\", style=\"Bold.TLabel\")\n",
    "preprocessing_frame = ttk.LabelFrame(thesis_gui, \n",
    "                         labelwidget = frame_label,\n",
    "                         relief='ridge')\n",
    "preprocessing_frame.pack(fill=\"both\",padx = 5, pady= 5)\n",
    "\n",
    "step_1 = StringVar()\n",
    "step_1.set('')\n",
    "#label\n",
    "extraction_label = Label(preprocessing_frame, width=27,\n",
    "                         text = \"Extraction\", \n",
    "                         font = (\"Helvetica 10 bold\"),\n",
    "                         bg = 'light blue',relief = 'ridge')\n",
    "extraction_label.grid(row=0,column=0,padx = 5, pady= 7)\n",
    "#entry\n",
    "extraction_entry = Entry(preprocessing_frame, width = 104, \n",
    "                         font = (\"Helvetica 12 \"),\n",
    "                         bg = 'white',\n",
    "                         textvariable = step_1, state = 'readonly', readonlybackground = 'white')\n",
    "extraction_entry.grid(row=0,column=1,padx = 5, pady= 7, columnspan=2)\n",
    "\n",
    "step_2 = StringVar()\n",
    "step_2.set('')\n",
    "#label\n",
    "deidentification_label = Label(preprocessing_frame, width=27,\n",
    "                         text = \"Data De-Identification\", \n",
    "                         font = (\"Helvetica 10 bold\"),\n",
    "                         bg = 'light blue',relief = 'ridge')\n",
    "deidentification_label.grid(row=1,column=0,padx = 5, pady= 7)\n",
    "#entry\n",
    "deidentification_entry = Entry(preprocessing_frame, width = 104, \n",
    "                         font = (\"Helvetica 12\"),\n",
    "                         bg = 'white',\n",
    "                         textvariable = step_2, state = 'readonly', readonlybackground = 'white')\n",
    "deidentification_entry.grid(row=1,column=1,padx = 5, pady= 7, columnspan=2)\n",
    "\n",
    "step_3 = StringVar()\n",
    "step_3.set('')\n",
    "#label\n",
    "url_label = Label(preprocessing_frame, width=27,\n",
    "                         text = \"URL Removal\", \n",
    "                         font = (\"Helvetica 10 bold\"),\n",
    "                         bg = 'light blue',relief = 'ridge')\n",
    "url_label.grid(row=2,column=0,padx = 5, pady= 7)\n",
    "#entry\n",
    "url_entry = Entry(preprocessing_frame, width = 104, \n",
    "                         font = (\"Helvetica 12\"),\n",
    "                         bg = 'white',\n",
    "                         textvariable = step_3, state = 'readonly', readonlybackground = 'white')\n",
    "url_entry.grid(row=2,column=1,padx = 5, pady= 7, columnspan=2)\n",
    "\n",
    "\n",
    "step_4 = StringVar()\n",
    "step_4.set('')\n",
    "#label\n",
    "specialcharacter_label = Label(preprocessing_frame, width=27,\n",
    "                         text = \"Special Character Processing\", \n",
    "                         font = (\"Helvetica 10 bold\"),\n",
    "                         bg = 'light blue',relief = 'ridge')\n",
    "specialcharacter_label.grid(row=3,column=0,padx = 5, pady= 7)\n",
    "#entry\n",
    "specialcharacter_entry = Entry(preprocessing_frame, width = 104, \n",
    "                         font = (\"Helvetica 12\"),\n",
    "                         bg = 'white',\n",
    "                         textvariable = step_4, state = 'readonly', readonlybackground = 'white')\n",
    "specialcharacter_entry.grid(row=3,column=1,padx = 5, pady= 7, columnspan=2)\n",
    "\n",
    "\n",
    "step_5 = StringVar()\n",
    "step_5.set('')\n",
    "#label\n",
    "normalization_label = Label(preprocessing_frame, width=27,\n",
    "                         text = \"Normalization\", \n",
    "                         font = (\"Helvetica 10 bold\"),\n",
    "                         bg = 'light blue',relief = 'ridge')\n",
    "normalization_label.grid(row=4,column=0,padx = 5, pady= 7)\n",
    "#entry\n",
    "normalization_entry = Entry(preprocessing_frame, width = 104, \n",
    "                         font = (\"Helvetica 12\"),\n",
    "                         bg = 'white',\n",
    "                         textvariable = step_5, state = 'readonly', readonlybackground = 'white')\n",
    "normalization_entry.grid(row=4,column=1,padx = 5, pady= 7, columnspan=2)\n",
    "\n",
    "step_6 = StringVar()\n",
    "step_6.set('')\n",
    "#label\n",
    "hash_label = Label(preprocessing_frame, width=27,\n",
    "                         text = \"Hashtag Processing\", \n",
    "                         font = (\"Helvetica 10 bold\"),\n",
    "                         bg = 'light blue',relief = 'ridge')\n",
    "hash_label.grid(row=5,column=0,padx = 5, pady= 7)\n",
    "#entry\n",
    "hash_entry = Entry(preprocessing_frame, width = 104, \n",
    "                         font = (\"Helvetica 12\"),\n",
    "                         bg = 'white',\n",
    "                         textvariable = step_6, state = 'readonly', readonlybackground = 'white')\n",
    "hash_entry.grid(row=5,column=1,padx = 5, pady= 7, columnspan=2)\n",
    "\n",
    "step_7 = StringVar()\n",
    "step_7.set('')\n",
    "#label\n",
    "tokenization_label = Label(preprocessing_frame, width=27,\n",
    "                         text = \"Tokenization\", \n",
    "                         font = (\"Helvetica 10 bold\"),\n",
    "                         bg = 'light blue',relief = 'ridge')\n",
    "tokenization_label.grid(row=6,column=0,padx = 5, pady= 7)\n",
    "#entry\n",
    "tokenization_entry = Entry(preprocessing_frame, width = 104, \n",
    "                         font = (\"Helvetica 12\"),\n",
    "                         bg = 'white',\n",
    "                         textvariable = step_7, state = 'readonly', readonlybackground = 'white')\n",
    "tokenization_entry.grid(row=6,column=1,padx = 5, pady= 7, columnspan=2)\n",
    "\n",
    "\n",
    "#classification frame\n",
    "frame_label = ttk.Label(text=\"CLASSIFICATION\", style=\"Bold.TLabel\")\n",
    "classification_frame = ttk.LabelFrame(thesis_gui, \n",
    "                         labelwidget = frame_label,\n",
    "                         relief='ridge')\n",
    "classification_frame.pack(fill=\"both\",expand = 'yes', padx = 5, pady= 5)\n",
    "\n",
    "#binary classification\n",
    "frame_label = ttk.Label(text=\"Binary Classification\", style=\"Bold2.TLabel\")\n",
    "binary_frame = ttk.LabelFrame(classification_frame, \n",
    "                         labelwidget = frame_label,\n",
    "                         relief='sunken')\n",
    "binary_frame.pack(side = 'left',fill=\"both\",expand = 'yes', padx = 5, pady= 5)\n",
    "\n",
    "#binary 7030\n",
    "\n",
    "frame_label = ttk.Label(text=\"70:30 Split\", style=\"Bold3.TLabel\")\n",
    "frame_7030 = ttk.LabelFrame(binary_frame, \n",
    "                         labelwidget = frame_label,\n",
    "                         relief='sunken')\n",
    "frame_7030.pack(fill=\"both\",expand = 'yes', padx = 5, pady= 5)\n",
    "\n",
    "#bf for binary fastText, bt for binary TFIDF\n",
    "bf_7030 = StringVar()\n",
    "bf_7030.set('')\n",
    "bt_7030 = StringVar()\n",
    "bt_7030.set('')\n",
    "#label\n",
    "\n",
    "bt_7030_label = Label(frame_7030, width=15,\n",
    "                         text = \"TFIDF FFNN\", \n",
    "                         font = (\"Helvetica 10\"),relief = 'ridge',\n",
    "                         bg = 'light yellow')\n",
    "bt_7030_label.grid(row=0,column=0,padx = 5, pady= 7)\n",
    "#classification\n",
    "bt_7030_prediction = Label(frame_7030, width = 51, \n",
    "                         font = (\"Helvetica 10\"),\n",
    "                         bg = 'white', relief = 'ridge',\n",
    "                         textvariable = bt_7030)\n",
    "bt_7030_prediction.grid(row=0,column=1,padx = 5, pady= 7)\n",
    "\n",
    "#label\n",
    "bf_7030_label = Label(frame_7030, width=15,\n",
    "                         text = \"fastText CNN\", \n",
    "                         font = (\"Helvetica 10\"),relief = 'ridge',\n",
    "                         bg = 'light pink')\n",
    "bf_7030_label.grid(row=1,column=0,padx = 5, pady= 7)\n",
    "#classification\n",
    "bf_7030_prediction = Label(frame_7030, width = 51, \n",
    "                         font = (\"Helvetica 10\"),\n",
    "                         bg = 'white', relief = 'ridge',\n",
    "                         textvariable = bf_7030)\n",
    "bf_7030_prediction.grid(row=1,column=1,padx = 5, pady= 7)\n",
    "\n",
    "#binary 8020\n",
    "bf_8020 = StringVar()\n",
    "bf_8020.set('')\n",
    "bt_8020 = StringVar()\n",
    "bt_8020.set('')\n",
    "\n",
    "frame_label = ttk.Label(text=\"80:20 Split\", style=\"Bold3.TLabel\")\n",
    "frame_8020 = ttk.LabelFrame(binary_frame, \n",
    "                         labelwidget = frame_label,\n",
    "                         relief='sunken')\n",
    "frame_8020.pack(fill=\"both\",expand = 'yes', padx = 5, pady= 5)\n",
    "\n",
    "bt_8020_label = Label(frame_8020, width=15,\n",
    "                         text = \"TFIDF FFNN\", \n",
    "                         font = (\"Helvetica 10\"),relief = 'ridge',\n",
    "                         bg = 'light yellow')\n",
    "bt_8020_label.grid(row=0,column=0,padx = 5, pady= 7)\n",
    "#classification\n",
    "bt_8020_prediction = Label(frame_8020, width = 51, \n",
    "                         font = (\"Helvetica 10\"),\n",
    "                         bg = 'white', relief = 'ridge',\n",
    "                         textvariable = bt_8020)\n",
    "bt_8020_prediction.grid(row=0,column=1,padx = 5, pady= 7)\n",
    "\n",
    "#label\n",
    "bf_8020_label = Label(frame_8020, width=15,\n",
    "                         text = \"fastText CNN\", \n",
    "                         font = (\"Helvetica 10\"),relief = 'ridge',\n",
    "                         bg = 'light pink')\n",
    "bf_8020_label.grid(row=1,column=0,padx = 5, pady= 7)\n",
    "#classification\n",
    "bf_8020_prediction = Label(frame_8020, width = 51, \n",
    "                         font = (\"Helvetica 10\"),\n",
    "                         bg = 'white', relief = 'ridge',\n",
    "                         textvariable = bf_8020)\n",
    "bf_8020_prediction.grid(row=1,column=1,padx = 5, pady= 7)\n",
    "\n",
    "#binary 9010\n",
    "bf_9010 = StringVar()\n",
    "bf_9010.set('')\n",
    "bt_9010 = StringVar()\n",
    "bt_9010.set('')\n",
    "\n",
    "frame_label = ttk.Label(text=\"90:10 Split\", style=\"Bold3.TLabel\")\n",
    "frame_9010 = ttk.LabelFrame(binary_frame, \n",
    "                         labelwidget = frame_label,\n",
    "                         relief='sunken')\n",
    "frame_9010.pack(fill=\"both\",expand = 'yes', padx = 5, pady= 5)\n",
    "\n",
    "bt_9010_label = Label(frame_9010, width=15,\n",
    "                         text = \"TFIDF FFNN\", \n",
    "                         font = (\"Helvetica 10\"),relief = 'ridge',\n",
    "                         bg = 'light yellow')\n",
    "bt_9010_label.grid(row=0,column=0,padx = 5, pady= 7)\n",
    "#classification\n",
    "bt_9010_prediction = Label(frame_9010, width = 51, \n",
    "                         font = (\"Helvetica 10\"),\n",
    "                         bg = 'white', relief = 'ridge',\n",
    "                         textvariable = bt_9010)\n",
    "bt_9010_prediction.grid(row=0,column=1,padx = 5, pady= 7)\n",
    "\n",
    "#label\n",
    "bf_9010_label = Label(frame_9010, width=15,\n",
    "                         text = \"fastText CNN\", \n",
    "                         font = (\"Helvetica 10\"),relief = 'ridge',\n",
    "                         bg = 'light pink')\n",
    "bf_9010_label.grid(row=1,column=0,padx = 5, pady= 7)\n",
    "#classification\n",
    "bf_9010_prediction = Label(frame_9010, width = 51, \n",
    "                         font = (\"Helvetica 10\"),\n",
    "                         bg = 'white', relief = 'ridge',\n",
    "                         textvariable = bf_9010)\n",
    "bf_9010_prediction.grid(row=1,column=1,padx = 5, pady= 7)\n",
    "\n",
    " \n",
    "#multilabel classification\n",
    "frame_label = ttk.Label(text=\"Multi label Classification\", style=\"Bold2.TLabel\")\n",
    "multilabel_frame = ttk.LabelFrame(classification_frame, \n",
    "                         labelwidget = frame_label,\n",
    "                         relief='sunken')\n",
    "multilabel_frame.pack(side = 'right',fill=\"both\",expand = 'yes', padx = 5, pady= 5)\n",
    "\n",
    "#variables\n",
    "mf_7030m = StringVar()\n",
    "mf_7030m.set('')\n",
    "mt_7030m = StringVar()\n",
    "mt_7030m.set('')\n",
    "mf_8020m = StringVar()\n",
    "mf_8020m.set('')\n",
    "mt_8020m = StringVar()\n",
    "mt_8020m.set('')\n",
    "mf_9010m = StringVar()\n",
    "mf_9010m.set('')\n",
    "mt_9010m = StringVar()\n",
    "mt_9010m.set('')\n",
    "\n",
    "#multilabel 7030\n",
    "frame_label = ttk.Label(text=\"70:30 Split\", style=\"Bold3.TLabel\")\n",
    "frame_7030m = ttk.LabelFrame(multilabel_frame, \n",
    "                         labelwidget = frame_label,\n",
    "                         relief='sunken')\n",
    "frame_7030m.pack(fill=\"both\",expand = 'yes', padx = 5, pady= 5)\n",
    "\n",
    "#label\n",
    "mt_7030_label = Label(frame_7030m, width=15,\n",
    "                         text = \"TFIDF FFNN\", \n",
    "                         font = (\"Helvetica 10\"), relief = 'ridge',\n",
    "                         bg = 'light yellow')\n",
    "mt_7030_label.grid(row=0,column=0,padx = 5, pady= 7)\n",
    "#entry\n",
    "mt_7030_prediction = Label(frame_7030m, width = 51, \n",
    "                         font = (\"Helvetica 10\"), relief = 'ridge',\n",
    "                         bg = 'white',\n",
    "                         textvariable = mt_7030m)\n",
    "mt_7030_prediction.grid(row=0,column=1,padx = 5, pady= 7)\n",
    "\n",
    "#label\n",
    "mf_7030_label = Label(frame_7030m, width=15,\n",
    "                         text = \"fastText CNN\", \n",
    "                         font = (\"Helvetica 10\"), relief = 'ridge',\n",
    "                         bg = 'light pink')\n",
    "mf_7030_label.grid(row=1,column=0,padx = 5, pady= 7)\n",
    "#entry\n",
    "mf_7030_prediction = Label(frame_7030m, width = 51, \n",
    "                         font = (\"Helvetica 10\"), relief = 'ridge',\n",
    "                         bg = 'white',\n",
    "                         textvariable = mf_7030m)\n",
    "mf_7030_prediction.grid(row=1,column=1,padx = 5, pady= 7)\n",
    "\n",
    "#multilabel 8020\n",
    "frame_label = ttk.Label(text=\"80:20 Split\", style=\"Bold3.TLabel\")\n",
    "frame_8020m = ttk.LabelFrame(multilabel_frame, \n",
    "                         labelwidget = frame_label,\n",
    "                         relief='sunken')\n",
    "frame_8020m.pack(fill=\"both\",expand = 'yes', padx = 5, pady= 5)\n",
    "\n",
    "#label\n",
    "mt_8020_label = Label(frame_8020m, width=15,\n",
    "                         text = \"TFIDF FFNN\", \n",
    "                         font = (\"Helvetica 10\"), relief = 'ridge',\n",
    "                         bg = 'light yellow')\n",
    "mt_8020_label.grid(row=0,column=0,padx = 5, pady= 7)\n",
    "#entry\n",
    "mt_8020_prediction = Label(frame_8020m, width = 51, \n",
    "                         font = (\"Helvetica 10\"), relief = 'ridge',\n",
    "                         bg = 'white',\n",
    "                         textvariable = mt_8020m)\n",
    "mt_8020_prediction.grid(row=0,column=1,padx = 5, pady= 7)\n",
    "\n",
    "#label\n",
    "mf_8020_label = Label(frame_8020m, width=15,\n",
    "                         text = \"fastText CNN\", \n",
    "                         font = (\"Helvetica 10\"), relief = 'ridge',\n",
    "                         bg = 'light pink')\n",
    "mf_8020_label.grid(row=1,column=0,padx = 5, pady= 7)\n",
    "#entry\n",
    "mf_8020_prediction = Label(frame_8020m, width = 51, \n",
    "                         font = (\"Helvetica 10\"), relief = 'ridge',\n",
    "                         bg = 'white',\n",
    "                         textvariable = mf_8020m)\n",
    "mf_8020_prediction.grid(row=1,column=1,padx = 5, pady= 7)\n",
    "\n",
    "#multilabel 9010\n",
    "frame_label = ttk.Label(text=\"90:10 Split\", style=\"Bold3.TLabel\")\n",
    "frame_9010m = ttk.LabelFrame(multilabel_frame, \n",
    "                         labelwidget = frame_label,\n",
    "                         relief='sunken')\n",
    "frame_9010m.pack(fill=\"both\",expand = 'yes', padx = 5, pady= 5)\n",
    "\n",
    "#label\n",
    "mt_9010_label = Label(frame_9010m, width=15,\n",
    "                         text = \"TFIDF FFNN\", \n",
    "                         font = (\"Helvetica 10\"), relief = 'ridge',\n",
    "                         bg = 'light yellow')\n",
    "mt_9010_label.grid(row=0,column=0,padx = 5, pady= 7)\n",
    "#entry\n",
    "mt_9010_prediction = Label(frame_9010m, width = 51, \n",
    "                         font = (\"Helvetica 10\"), relief = 'ridge',\n",
    "                         bg = 'white',\n",
    "                         textvariable = mt_9010m)\n",
    "mt_9010_prediction.grid(row=0,column=1,padx = 5, pady= 7)\n",
    "\n",
    "#label\n",
    "mf_9010_label = Label(frame_9010m, width=15,\n",
    "                         text = \"fastText CNN\", \n",
    "                         font = (\"Helvetica 10\"), relief = 'ridge',\n",
    "                         bg = 'light pink')\n",
    "mf_9010_label.grid(row=1,column=0,padx = 5, pady= 7)\n",
    "#entry\n",
    "\n",
    "mf_9010_prediction = Label(frame_9010m, width = 51, \n",
    "                         font = (\"Helvetica 10 \"), relief = 'ridge',\n",
    "                         bg = 'white',\n",
    "                         textvariable = mf_9010m)\n",
    "mf_9010_prediction.grid(row=1,column=1,padx = 5, pady= 7)\n",
    "\n",
    "#buttons\n",
    "clear_button = ttk.Button(thesis_gui, text=\"Clear\", \n",
    "                             width=\"20\",command = clear, style = \"Bold.TButton\")\n",
    "clear_button.pack(side='left',padx=5,pady=3, fill='both', expand = 'yes')\n",
    "\n",
    "classify_button = ttk.Button(thesis_gui, text=\"Classify Tweet\", \n",
    "                             width=\"20\",command = classify, style = \"Bold.TButton\")\n",
    "classify_button.pack(side='right',padx=5,pady=3, fill='both', expand = 'yes')\n",
    "\n",
    "#run gui\n",
    "thesis_gui.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be759ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa9f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
